{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "word2vec_continuous_bag_of_words.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrjC44FA3r0J"
      },
      "source": [
        "import spacy\n",
        "import tqdm\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch import nn\n",
        "from collections import Counter\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hoKO5ZMj9BmP"
      },
      "source": [
        "spacy_eng = spacy.load('en')\n",
        "def tokenizer_eng(text):\n",
        "    return [t.text for t in spacy_eng.tokenizer(text)]"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HPUXXlGT7keq",
        "outputId": "c3697d21-037d-4fd6-ac87-d15b92a0a011"
      },
      "source": [
        "text = 'what is your name?'\n",
        "tokenizer_eng(text)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['what', 'is', 'your', 'name', '?']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tEx8AyJR4BK5"
      },
      "source": [
        "class GetDataset(Dataset):\n",
        "    def __init__(self, file_name, window_size=2):\n",
        "        self.window_size = window_size\n",
        "        raw_text = open(file_name, 'r', encoding='utf-8').read().lower()\n",
        "        tokenized_words = tokenizer_eng(raw_text)\n",
        "        self.data_pairs = [\n",
        "                      (\n",
        "                          [tokenized_words[i-(j+1)] for j in range(window_size)] + [tokenized_words[i+(j+1)] for j in range(window_size)], tokenized_words[i]\n",
        "                      ) for i in range(window_size, len(tokenized_words)-window_size)\n",
        "                    ]\n",
        "        self.vocab = Counter(tokenized_words)\n",
        "        self.vocab_size = len(self.vocab)\n",
        "        self.stoi = {item[0]: idx for idx, item in enumerate(self.vocab.most_common())}\n",
        "        self.itos = list(self.stoi.keys())\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_pairs)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        context = torch.LongTensor([self.stoi[w] for w in self.data_pairs[index][0]])\n",
        "        target = torch.tensor(self.stoi[self.data_pairs[index][1]])\n",
        "        return context, target"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rF8FhsHn6LFw"
      },
      "source": [
        "class CBOW(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_size):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
        "        self.fc = nn.Sequential(\n",
        "                        nn.Linear(embedding_size, 128),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Linear(128, 256),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Linear(256, 512),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Linear(512, vocab_size)\n",
        "                    )        \n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x).sum(1) # (b, t, d) -> (b, d)\n",
        "        return self.fc(x)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1RyesClZ8s2u",
        "outputId": "fef5e561-3d29-472f-9255-d54114561521"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "window_size = 2\n",
        "n_epochs = 20\n",
        "embedding_size = 1028\n",
        "batch_size = 64\n",
        "lr = 3e-4\n",
        "print(device)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fRvIqg2Y5cMl",
        "outputId": "4dde380f-f052-4d0c-98f4-b5df5a5ad4d8"
      },
      "source": [
        "data = GetDataset('input.txt', window_size)\n",
        "loader = DataLoader(data, batch_size=batch_size, num_workers=2, pin_memory=True, shuffle=True)\n",
        "x, y = next(iter(loader))\n",
        "print(len(data), data.vocab_size, x.shape, y.shape)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "287756 12340 torch.Size([64, 4]) torch.Size([64])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jMVrZm1G7LUR",
        "outputId": "5dc73e4f-eeaf-45e7-cf9c-a0f30908597c"
      },
      "source": [
        "net = CBOW(data.vocab_size, embedding_size).to(device)\n",
        "inp = torch.LongTensor([[0, 1, 2, 3]]).to(device)\n",
        "out = net(inp)\n",
        "print(out.shape)\n",
        "del inp, out\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "loss_fn = nn.CrossEntropyLoss()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 12340])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FpkfZKwn9I40"
      },
      "source": [
        "def loop(net, loader, epoch):\n",
        "    net.train()\n",
        "    losses = []\n",
        "    pbar = tqdm.tqdm(loader, total=len(loader))\n",
        "    for x, y in pbar:\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "        preds = net(x)\n",
        "        loss = loss_fn(preds, y)\n",
        "        losses.append(loss.item())\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        pbar.set_description(f'epoch={epoch}, avg_loss={np.mean(losses):.4f}')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eLPi5j0i-2jn",
        "outputId": "3f4eb884-c081-4ec4-8af0-faf8a7461101"
      },
      "source": [
        "for epoch in range(n_epochs):\n",
        "    loop(net, loader, epoch)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch=0, avg_loss=5.3158: 100%|██████████| 4497/4497 [00:42<00:00, 105.01it/s]\n",
            "epoch=1, avg_loss=4.7350: 100%|██████████| 4497/4497 [00:42<00:00, 105.30it/s]\n",
            "epoch=2, avg_loss=4.3994: 100%|██████████| 4497/4497 [00:42<00:00, 105.25it/s]\n",
            "epoch=3, avg_loss=4.1293: 100%|██████████| 4497/4497 [00:42<00:00, 105.00it/s]\n",
            "epoch=4, avg_loss=3.8954: 100%|██████████| 4497/4497 [00:42<00:00, 105.05it/s]\n",
            "epoch=5, avg_loss=3.6806: 100%|██████████| 4497/4497 [00:42<00:00, 105.23it/s]\n",
            "epoch=6, avg_loss=3.4775: 100%|██████████| 4497/4497 [00:42<00:00, 105.40it/s]\n",
            "epoch=7, avg_loss=3.2823: 100%|██████████| 4497/4497 [00:42<00:00, 105.28it/s]\n",
            "epoch=8, avg_loss=3.0992: 100%|██████████| 4497/4497 [00:42<00:00, 105.22it/s]\n",
            "epoch=9, avg_loss=2.9342: 100%|██████████| 4497/4497 [00:42<00:00, 105.33it/s]\n",
            "epoch=10, avg_loss=2.7922: 100%|██████████| 4497/4497 [00:42<00:00, 105.22it/s]\n",
            "epoch=11, avg_loss=2.6720: 100%|██████████| 4497/4497 [00:42<00:00, 105.07it/s]\n",
            "epoch=12, avg_loss=2.5684: 100%|██████████| 4497/4497 [00:42<00:00, 105.25it/s]\n",
            "epoch=13, avg_loss=2.4750: 100%|██████████| 4497/4497 [00:42<00:00, 105.38it/s]\n",
            "epoch=14, avg_loss=2.3917: 100%|██████████| 4497/4497 [00:42<00:00, 105.29it/s]\n",
            "epoch=15, avg_loss=2.3151: 100%|██████████| 4497/4497 [00:42<00:00, 105.17it/s]\n",
            "epoch=16, avg_loss=2.2448: 100%|██████████| 4497/4497 [00:42<00:00, 105.29it/s]\n",
            "epoch=17, avg_loss=2.1810: 100%|██████████| 4497/4497 [00:42<00:00, 105.07it/s]\n",
            "epoch=18, avg_loss=2.1208: 100%|██████████| 4497/4497 [00:42<00:00, 104.83it/s]\n",
            "epoch=19, avg_loss=2.0623: 100%|██████████| 4497/4497 [00:43<00:00, 104.51it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_ePgORw_0Vu"
      },
      "source": [
        "def get_similar_words(word, data):\n",
        "    if word not in data.stoi:\n",
        "        raise Exception('word not found in the vocab!!')\n",
        "    word_idx = torch.LongTensor([data.stoi[word]]).to(device)\n",
        "    word_embedding = net.embedding(word_idx)\n",
        "    similar_words = []\n",
        "    \n",
        "    for curr_word in data.vocab:\n",
        "        if curr_word == word: \n",
        "            continue\n",
        "        curr_idx = torch.LongTensor([data.stoi[curr_word]]).to(device)\n",
        "        curr_embedding = net.embedding(curr_idx)\n",
        "        cosine_sim = F.cosine_similarity(word_embedding, curr_embedding)\n",
        "        similar_words.append([curr_word, cosine_sim.item()])\n",
        "\n",
        "    return sorted(similar_words, key=lambda x: x[1], reverse=True)[:10]"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bcb_XoqdCJXj",
        "outputId": "1506af81-341f-433b-cdab-541de18f5187"
      },
      "source": [
        "get_similar_words('what', data)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['excepting', 0.12064912170171738],\n",
              " ['bon', 0.11391300708055496],\n",
              " ['foremost', 0.1138283982872963],\n",
              " ['volumnia', 0.11017448455095291],\n",
              " ['devouring', 0.10938507318496704],\n",
              " [\"view'd\", 0.1080944836139679],\n",
              " ['descends', 0.10349922627210617],\n",
              " ['hasty', 0.1014949157834053],\n",
              " ['gory', 0.1014704555273056],\n",
              " ['unseemly', 0.10107247531414032]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YYfOIAHfGzkf",
        "outputId": "d2c383b7-6cb5-4600-b1c4-bc04c5cad506"
      },
      "source": [
        "get_similar_words('dog', data)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['troth', 0.11910170316696167],\n",
              " ['kindest', 0.11287535727024078],\n",
              " ['prisoner', 0.11184903979301453],\n",
              " ['capers', 0.10974440723657608],\n",
              " ['den', 0.10471750050783157],\n",
              " ['characters', 0.10162997245788574],\n",
              " ['smell', 0.10123172402381897],\n",
              " ['answering', 0.10066694766283035],\n",
              " ['allegiance', 0.1002424955368042],\n",
              " ['uncrown', 0.09983837604522705]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "VlvXyngkG54y",
        "outputId": "5b2de351-9b98-4347-ec37-0e5a7d9457e3"
      },
      "source": [
        "get_similar_words('asdas', data)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Exception",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-0e74984f376e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_similar_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'asdas'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-23-2164e3bbd51e>\u001b[0m in \u001b[0;36mget_similar_words\u001b[0;34m(word, data)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_similar_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstoi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'word not found in the vocab!!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mword_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstoi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mword_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mException\u001b[0m: word not found in the vocab!!"
          ]
        }
      ]
    }
  ]
}