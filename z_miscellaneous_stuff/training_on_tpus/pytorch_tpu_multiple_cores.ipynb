{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mk5chvX6_7PR"
   },
   "outputs": [],
   "source": [
    "!pip install cloud-tpu-client==0.10 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.9-cp37-cp37m-linux_x86_64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MK0mizhv8cDY",
    "outputId": "a94f9a4b-5ded-4028-8798-5b5c15c0ca68"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:TPU has started up successfully with version pytorch-1.9\n"
     ]
    }
   ],
   "source": [
    "import torch_xla\n",
    "import torch_xla.core.xla_model as xm\n",
    "import torch_xla.distributed.parallel_loader as dpl\n",
    "import torch_xla.distributed.xla_multiprocessing as xmp\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms, datasets, models\n",
    "from torch.utils.data import random_split, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "PG29mRuw8_dg"
   },
   "outputs": [],
   "source": [
    "# use brain float16 datatype\n",
    "!export XLA_USE_BF16=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "5c-eal-A8cA1"
   },
   "outputs": [],
   "source": [
    "serial_exec = xmp.MpSerialExecutor()\n",
    "flags = {\n",
    "    'n_epochs': 1,\n",
    "    'batch_size': 64, # batch_size will be scaled by num_cores times\n",
    "    'lr': 3e-4,\n",
    "    'num_cores': 8\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ViSIEUK28b-T"
   },
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    T = transforms.Compose(\n",
    "        [\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor()\n",
    "        ]\n",
    "    )\n",
    "    data = datasets.CIFAR10(\"data/\", train=True, download=False, transform=T)\n",
    "    test_data = datasets.CIFAR10(\"data/\", train=False, download=False, transform=T)\n",
    "    val_len = int(0.3 * len(data))\n",
    "    train_data, val_data = random_split(data, [len(data) - val_len, val_len])\n",
    "    return train_data, val_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "fSoaX8H6AeF5"
   },
   "outputs": [],
   "source": [
    "model = models.resnet18(pretrained=False)\n",
    "model.fc = nn.Linear(model.fc.in_features, 10)\n",
    "# model = xmp.MpModelWrapper(model) # not necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "eQnDOPV4vgBS"
   },
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "def get_accuracy(preds, y):\n",
    "    preds = preds.argmax(1)\n",
    "    num_correct = (preds == y).sum().item()\n",
    "    acc = num_correct / y.shape[0]\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "MAu85kSC8b3X"
   },
   "outputs": [],
   "source": [
    "def loop(net, loader, is_train, optimizer=None):\n",
    "    net.train(is_train)\n",
    "    losses = []\n",
    "    accs = []\n",
    "    # if is_train:\n",
    "    #     split = 'train'\n",
    "    # else:\n",
    "    #     split = ' val '\n",
    "\n",
    "    # pbar = tqdm(loader, total=len(loader)) # tqdm bar is kindof glitchy because the data is split across multiple cores\n",
    "    for x, y in loader:\n",
    "        with torch.set_grad_enabled(is_train):\n",
    "            preds = net(x)\n",
    "            loss = loss_fn(preds, y)\n",
    "            acc = get_accuracy(preds, y)\n",
    "            losses.append(loss.item())\n",
    "            accs.append(acc)\n",
    "\n",
    "        if is_train:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            xm.optimizer_step(optimizer) # tpu-specific code\n",
    "        \n",
    "        # if epoch != None:\n",
    "        #     pbar.set_description(f'{split}: epoch={epoch}, loss={np.mean(losses):.4f}, acc={np.mean(accs):.4f}')\n",
    "        # else:\n",
    "        #     pbar.set_description(f'loss={np.mean(losses):.4f}, acc={np.mean(accs):.4f}')\n",
    "    \n",
    "    return np.mean(losses), np.mean(accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "nTEOiy3S9lQP"
   },
   "outputs": [],
   "source": [
    "def run(flags):\n",
    "    xm.master_print('grabbing the data...') # usual print prints individual outputs from all the cores, master_print prints only one output which is gathered from all the cores\n",
    "    train_data, val_data, test_data = serial_exec.run(get_data)\n",
    "\n",
    "    xm.master_print('creating the dataloaders...')\n",
    "    # xm.get_ordinal(): current core, xm.xrt_world_size(): num cores\n",
    "    train_sampler = torch.utils.data.distributed.DistributedSampler(train_data, num_replicas=xm.xrt_world_size(), rank=xm.get_ordinal(), shuffle=True)\n",
    "    train_loader = DataLoader(train_data, batch_size=flags['batch_size'], sampler=train_sampler, drop_last=True)\n",
    "\n",
    "    val_sampler = torch.utils.data.distributed.DistributedSampler(val_data, num_replicas=xm.xrt_world_size(), rank=xm.get_ordinal(), shuffle=False)\n",
    "    val_loader = DataLoader(val_data, batch_size=flags['batch_size'], sampler=val_sampler)\n",
    "\n",
    "    test_sampler = torch.utils.data.distributed.DistributedSampler(test_data, num_replicas=xm.xrt_world_size(), rank=xm.get_ordinal(), shuffle=False)\n",
    "    test_loader = DataLoader(test_data, batch_size=flags['batch_size'], sampler=test_sampler)\n",
    "\n",
    "    device = xm.xla_device()\n",
    "    xm.master_print('creating the model...')\n",
    "    curr_model = model.to(device)\n",
    "    new_lr = flags['lr'] * xm.xrt_world_size() # coz the batch_size is scaled xm.xrt_world_size() times\n",
    "    optimizer = torch.optim.Adam(curr_model.parameters(), lr=new_lr)\n",
    "\n",
    "    xm.master_print('starting the training...')\n",
    "    for epoch in range(flags['n_epochs']):\n",
    "        train_para_loader = dpl.ParallelLoader(train_loader, [device])\n",
    "        train_loss, train_acc = loop(curr_model, train_para_loader.per_device_loader(device), True, optimizer)\n",
    "\n",
    "        val_para_loader = dpl.ParallelLoader(val_loader, [device])\n",
    "        val_loss, val_acc = loop(curr_model, val_para_loader.per_device_loader(device), False)\n",
    "\n",
    "        xm.master_print(f'epoch={epoch}, train_loss={train_loss:.4f}, train_acc={train_acc:.4f}, val_loss={val_loss:.4f}, val_acc={val_acc:.4f}')\n",
    "\n",
    "    test_para_loader = dpl.ParallelLoader(test_loader, [device])\n",
    "    test_loss, test_acc = loop(curr_model, test_para_loader.per_device_loader(device), False)\n",
    "    xm.master_print(f'test_loss={test_loss:.4f}, test_acc={test_acc:.4f}')\n",
    "\n",
    "    xm.master_print('saving the model weights...')\n",
    "    xm.save(curr_model.state_dict(), 'weights.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "_uIdHWxj8XiW"
   },
   "outputs": [],
   "source": [
    "def map_fn(rank, flags):\n",
    "    '''\n",
    "    rank: current tpu core\n",
    "    flags: training args\n",
    "    '''\n",
    "    torch.set_default_tensor_type('torch.FloatTensor')\n",
    "    run(flags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N_G2DABdf7vz",
    "outputId": "35221a8c-6409-4f02-e6ad-9a6116c75db8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grabbing the data...\n",
      "creating the dataloaders...\n",
      "creating the model...\n",
      "starting the training...\n",
      "epoch=0, train_loss=1.8077, train_acc=0.3297, val_loss=1.9125, val_acc=0.3456\n",
      "test_loss=1.8780, test_acc=0.3368\n",
      "saving the model weights...\n"
     ]
    }
   ],
   "source": [
    "xmp.spawn(map_fn, args=(flags,), nprocs=flags['num_cores'], start_method='fork')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XZMgBX777iAO",
    "outputId": "d98720f2-627c-4728-a102-99cb1202d594"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('weights.pth'))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "pytorch_tpu_multiple_cores.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
