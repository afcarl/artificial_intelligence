{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "default.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Byl87GUMSTiV"
      },
      "source": [
        "import cv2\r\n",
        "import gym\r\n",
        "import copy\r\n",
        "import tqdm\r\n",
        "import torch\r\n",
        "import math\r\n",
        "import random\r\n",
        "import numpy as np\r\n",
        "import torch.nn as nn\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import torch.nn.functional as F\r\n",
        "from collections import deque, namedtuple"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFBzDRkCGjOD"
      },
      "source": [
        "import warnings\r\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "OyxAwVwMGzRd",
        "outputId": "609cff1c-c818-4716-d0f8-61ab7f4de9e3"
      },
      "source": [
        "torch.cuda.get_device_name()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Tesla T4'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1MB9-sr_TjMi"
      },
      "source": [
        "SEED = 42"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2g1oDbwSTe6"
      },
      "source": [
        "torch.manual_seed(SEED)\r\n",
        "torch.cuda.manual_seed(SEED)\r\n",
        "torch.cuda.manual_seed_all(SEED)\r\n",
        "np.random.seed(SEED)\r\n",
        "random.seed(SEED)\r\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6GhmvA2ETq5I"
      },
      "source": [
        "class ReplayMemory():\r\n",
        "    def __init__(self, mem_capacity, batch_size):\r\n",
        "        self.mem_capacity = mem_capacity\r\n",
        "        self.batch_size = batch_size\r\n",
        "        self.memory = deque(maxlen=self.mem_capacity)\r\n",
        "        self.Transition = namedtuple('Transition', ('state', 'action', 'reward', 'next_state')) \r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return len(self.memory)\r\n",
        "\r\n",
        "    def push(self, s, a, r, ns):\r\n",
        "        s = torch.FloatTensor(s)\r\n",
        "        a = torch.LongTensor([a])\r\n",
        "        r = torch.FloatTensor([r])\r\n",
        "        if ns is not None:\r\n",
        "            ns = torch.FloatTensor(ns)\r\n",
        "\r\n",
        "        transition = self.Transition(state=s, action=a, reward=r, next_state=ns)\r\n",
        "        self.memory.append(transition)\r\n",
        "\r\n",
        "    def sample(self):\r\n",
        "        transitions = random.sample(self.memory, self.batch_size)\r\n",
        "        return self.Transition(*(zip(*transitions)))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Agqt_wbiSTcr"
      },
      "source": [
        "class SkipMax(gym.Wrapper):\r\n",
        "    def __init__(self, env, skip=4):\r\n",
        "        gym.Wrapper.__init__(self, env)\r\n",
        "        self.skip = skip\r\n",
        "        self.frame_buffer = np.zeros((2,) + env.observation_space.shape, dtype=np.uint8)\r\n",
        "\r\n",
        "    def reset(self):\r\n",
        "        return self.env.reset()\r\n",
        "\r\n",
        "    def step(self, action):\r\n",
        "        total_reward = 0\r\n",
        "        for i in range(self.skip):\r\n",
        "            state, reward, done, info = self.env.step(action)\r\n",
        "\r\n",
        "            if i == self.skip - 2:\r\n",
        "                self.frame_buffer[0] = state\r\n",
        "\r\n",
        "            if i == self.skip - 1:\r\n",
        "                self.frame_buffer[1] = state\r\n",
        "\r\n",
        "            total_reward += reward\r\n",
        "            if done:\r\n",
        "                break\r\n",
        "\r\n",
        "        max_frame = self.frame_buffer.max(axis=0)\r\n",
        "        return max_frame, total_reward, done, info"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfm3TDfwWl-D"
      },
      "source": [
        "class WrapFrame(gym.ObservationWrapper):\n",
        "    def __init__(self, env):\n",
        "        gym.ObservationWrapper.__init__(self, env)\n",
        "        self.width, self.height = 84, 84\n",
        "        self.observation_space = gym.spaces.Box(low=0, high=255, shape=(self.height, self.width, 1), dtype=np.uint8)\n",
        "\n",
        "    def observation(self, frame):\n",
        "        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
        "        frame = cv2.resize(frame, (self.width, self.height), interpolation=cv2.INTER_AREA)\n",
        "        return frame[:, :, None]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UbuBmtRxY9lw"
      },
      "source": [
        "class ClipReward(gym.RewardWrapper):\n",
        "    def __init__(self, env):\n",
        "        gym.RewardWrapper.__init__(self, env)\n",
        "\n",
        "    def reward(self, r):\n",
        "        return np.sign(r)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ogs4XEHpaijL"
      },
      "source": [
        "class LazyFrames(object):\n",
        "    def __init__(self, frames):\n",
        "        self.frames = frames\n",
        "        self.output = None\n",
        "\n",
        "    def get_output(self):\n",
        "        if self.output is None:\n",
        "            self.output = np.concatenate(self.frames, axis=2)\n",
        "            self.frames = None\n",
        "        \n",
        "        return self.output\n",
        "\n",
        "    def __array__(self, dtype=None):\n",
        "        output = self.get_output()\n",
        "        if dtype is not None:\n",
        "            output = output.astype(dtype)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.get_output())\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.get_output()[index]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwOFFkFcWl60"
      },
      "source": [
        "class StackFrames(gym.Wrapper):\r\n",
        "    def __init__(self, env, k=4):\r\n",
        "        gym.Wrapper.__init__(self, env)\r\n",
        "        self.k = k\r\n",
        "        self.frames = deque(maxlen=k)\r\n",
        "\r\n",
        "        obs_shape = env.observation_space.shape\r\n",
        "        self.observation_space = gym.spaces.Box(low=0, high=255, shape=(obs_shape[0], obs_shape[1], obs_shape[2] * k), dtype=np.uint8)\r\n",
        "    \r\n",
        "    def get_frames(self):\r\n",
        "        return LazyFrames(list(self.frames))\r\n",
        "\r\n",
        "    def reset(self):\r\n",
        "        obs = self.env.reset()\r\n",
        "        for _ in range(self.k):\r\n",
        "            self.frames.append(obs)        \r\n",
        "\r\n",
        "        return self.get_frames()\r\n",
        "\r\n",
        "    def step(self, action):\r\n",
        "        obs, reward, done, info = self.env.step(action)\r\n",
        "        self.frames.append(obs)\r\n",
        "\r\n",
        "        return self.get_frames(), reward, done, info"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMw0--IYSTaM"
      },
      "source": [
        "class WrapImage(gym.ObservationWrapper):\r\n",
        "    def __init__(self, env):\r\n",
        "        gym.ObservationWrapper.__init__(self, env)\r\n",
        "        obs_shape = env.observation_space.shape\r\n",
        "        self.observation_shape = gym.spaces.Box(low=0.0, high=1.0, shape=(obs_shape[-1], obs_shape[0], obs_shape[1]), dtype=np.uint8)\r\n",
        "\r\n",
        "    def observation(self, image):\r\n",
        "        return np.swapaxes(image, 2, 0)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uf07E0toral1"
      },
      "source": [
        "def get_env(env_name):\r\n",
        "    env = gym.make(env_name)\r\n",
        "    env = SkipMax(env)\r\n",
        "    env = WrapFrame(env)\r\n",
        "    env = ClipReward(env)\r\n",
        "    env = StackFrames(env)\r\n",
        "    env = WrapImage(env)\r\n",
        "    return env"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZuxshbFhraid"
      },
      "source": [
        "class DQN(nn.Module):\r\n",
        "    def __init__(self, output_size):\r\n",
        "        super().__init__()\r\n",
        "        self.conv1 = nn.Conv2d(4, 32, kernel_size=8, stride=4)\r\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\r\n",
        "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\r\n",
        "\r\n",
        "        self.fc1 = nn.Linear(7 * 7 * 64, 512)\r\n",
        "        self.fc2 = nn.Linear(512, output_size)\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        x = F.relu(self.conv1(x))\r\n",
        "        x = F.relu(self.conv2(x))\r\n",
        "        x = F.relu(self.conv3(x))\r\n",
        "        x = x.view(x.shape[0], -1)\r\n",
        "\r\n",
        "        x = F.relu(self.fc1(x))\r\n",
        "        x = self.fc2(x)\r\n",
        "\r\n",
        "        return x"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLVjcBhUs95Z"
      },
      "source": [
        "class Agent():\r\n",
        "    def __init__(self, env, net, memory, update_freq, learning_start, e_start, e_end, e_steps, gamma, target_update, print_every, render):\r\n",
        "        self.env = env\r\n",
        "        self.net = net.to(device)\r\n",
        "        self.memory = memory\r\n",
        "        self.update_freq = update_freq\r\n",
        "        self.learning_start = learning_start\r\n",
        "        self.e_start = e_start\r\n",
        "\r\n",
        "        self.e_end = e_end\r\n",
        "        self.e_steps = e_steps\r\n",
        "        self.gamma = gamma\r\n",
        "        self.target_update = target_update\r\n",
        "        self.print_every = print_every\r\n",
        "        self.render = render\r\n",
        "\r\n",
        "        self.steps = 0\r\n",
        "        self.episode = 0\r\n",
        "        self.target_net = copy.deepcopy(self.net)\r\n",
        "        self.target_net.eval()\r\n",
        "        self.opt = torch.optim.Adam(self.net.parameters(), lr=1e-4)\r\n",
        "        \r\n",
        "    def get_epsilon(self):\r\n",
        "        epsilon = self.e_end + (self.e_start - self.e_end) * math.exp(-1. * self.steps / self.e_steps)\r\n",
        "        return epsilon\r\n",
        "\r\n",
        "    def get_action(self, state):\r\n",
        "        epsilon = self.get_epsilon()\r\n",
        "        if random.random() < epsilon:\r\n",
        "            action = self.env.action_space.sample()\r\n",
        "        else:\r\n",
        "            with torch.no_grad():\r\n",
        "                state = torch.FloatTensor(state).unsqueeze(0).to(device)\r\n",
        "                Q = self.net(state)\r\n",
        "                action = Q.max(1)[1].item()\r\n",
        "\r\n",
        "        return action\r\n",
        "\r\n",
        "    def train(self, episodes):\r\n",
        "        ep_rewards = []\r\n",
        "\r\n",
        "        for episode in tqdm.tqdm(range(episodes), total=episodes):\r\n",
        "            done = False\r\n",
        "            episode_reward = 0\r\n",
        "            state = self.env.reset()\r\n",
        "\r\n",
        "            while not done:\r\n",
        "                if self.render == True and episode % self.print_every == 0:\r\n",
        "                    env.render()\r\n",
        "\r\n",
        "                action = self.get_action(state)\r\n",
        "                next_state, reward, done, _ = self.env.step(action)\r\n",
        "                episode_reward += reward\r\n",
        "                self.memory.push(state, action, reward, None if done else next_state)\r\n",
        "                state = next_state\r\n",
        "                self.steps += 1\r\n",
        "\r\n",
        "                if self.steps % self.update_freq == 0 and self.steps > self.learning_start:\r\n",
        "                    ep_loss = self.optimize()\r\n",
        "\r\n",
        "                if self.steps % (self.target_update * self.update_freq) == 0 and self.steps > self.learning_start:\r\n",
        "                    self.target_net.load_state_dict(self.net.state_dict())\r\n",
        "\r\n",
        "            ep_rewards.append(episode_reward)\r\n",
        "            if episode % self.print_every == 0:\r\n",
        "                avg_reward = np.mean(ep_rewards[-self.print_every:])\r\n",
        "                print(f\" episode: {episode} | avg_reward: {avg_reward:.4f}\")\r\n",
        "\r\n",
        "        return ep_rewards\r\n",
        "\r\n",
        "    def optimize(self):\r\n",
        "        mem_sample = self.memory.sample()\r\n",
        "        non_terminal_mask = torch.ByteTensor(list(map(lambda ns: ns is not None, mem_sample.next_state)))\r\n",
        "\r\n",
        "        state_batch = torch.cat(mem_sample.state).to(device)\r\n",
        "        action_batch = torch.cat(mem_sample.action).unsqueeze(1).to(device)\r\n",
        "        reward_batch = torch.cat(mem_sample.reward).unsqueeze(1).to(device)\r\n",
        "        non_terminal_next_state_batch = torch.cat([ns for ns in mem_sample.next_state if ns is not None]).to(device)\r\n",
        "\r\n",
        "        state_batch = state_batch.view(self.memory.batch_size, 4, 84, 84)\r\n",
        "        non_terminal_next_state_batch = non_terminal_next_state_batch.view(-1, 4, 84, 84)\r\n",
        "\r\n",
        "        Q_preds = self.net(state_batch)\r\n",
        "        Q_vals = Q_preds.gather(1, action_batch)\r\n",
        "\r\n",
        "        target_preds = self.target_net(non_terminal_next_state_batch)\r\n",
        "        target_vals = torch.zeros(self.memory.batch_size, 1).to(device)\r\n",
        "        target_vals[non_terminal_mask] = target_preds.max(1)[0].unsqueeze(1)\r\n",
        "        \r\n",
        "        expected_vals = reward_batch + (self.gamma * target_vals)\r\n",
        "        loss = F.smooth_l1_loss(Q_vals, expected_vals.detach())\r\n",
        "\r\n",
        "        self.opt.zero_grad()\r\n",
        "        loss.backward()\r\n",
        "        for p in self.net.parameters():\r\n",
        "            p.grad.data.clamp_(-1, 1)\r\n",
        "        self.opt.step()\r\n",
        "\r\n",
        "        return loss.item()"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ULzqtIptfMq"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n",
        "episodes = 500\r\n",
        "mem_capacity = 10000\r\n",
        "batch_size = 32\r\n",
        "render = False \r\n",
        "env_name = \"PongNoFrameskip-v4\"\r\n",
        "output_size = gym.make(env_name).action_space.n\r\n",
        "learning_start = 10000\r\n",
        "update_freq = 1\r\n",
        "e_start = 1.0\r\n",
        "e_end = 0.01\r\n",
        "e_steps = 30000\r\n",
        "gamma = 0.99\r\n",
        "target_update = 1000\r\n",
        "print_every = 50"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5p7QaYW1DLvk",
        "outputId": "d3aec6fa-ca85-4f61-edfe-a59979227130"
      },
      "source": [
        "device"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lv73mmM5s92I"
      },
      "source": [
        "env = get_env(env_name)\r\n",
        "env.seed(SEED)\r\n",
        "memory = ReplayMemory(mem_capacity, batch_size)\r\n",
        "net = DQN(output_size)\r\n",
        "agent = Agent(env, net, memory, update_freq, learning_start, e_start, e_end, \r\n",
        "              e_steps, gamma, target_update, print_every, render)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFqSOZKyCR7_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "outputId": "fe4612ea-67cf-4c60-c9b2-8ec94cda4dd7"
      },
      "source": [
        "reward_history = agent.train(episodes)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 1/500 [00:01<10:43,  1.29s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " episode: 0 | avg_reward: -21.0000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 10%|█         | 51/500 [07:53<1:45:01, 14.03s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " episode: 50 | avg_reward: -20.1800\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 20%|██        | 101/500 [26:22<2:45:44, 24.92s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " episode: 100 | avg_reward: -10.7600\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 20%|██        | 102/500 [26:46<2:43:07, 24.59s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-c33581bbd130>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mreward_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-15-88518d5800ae>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, episodes)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_freq\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_start\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m                     \u001b[0mep_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_update\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_freq\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_start\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-88518d5800ae>\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mnon_terminal_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mByteTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mns\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mns\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmem_sample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0mstate_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmem_sample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0maction_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmem_sample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mreward_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmem_sample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZeJotOi8LAT"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHGrUH1m8KXG"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}