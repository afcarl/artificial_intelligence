{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2WECboyUr4jd"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/TheAlgorithms/Python.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HRXkxpQ1peTK",
    "outputId": "fc5f3019-2b23-4d8e-8a11-e6e4b204710f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9.0+cu102 10.2 4.7.0\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import transformers\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, random_split, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorWithPadding, AdamW, get_scheduler\n",
    "# AutoModelWithLMHead is deprecated\n",
    "\n",
    "def clear_cache():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "clear_cache()\n",
    "seed_everything(42)\n",
    "print(torch.__version__, torch.version.cuda, transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oMGJo49QTU3K",
    "outputId": "71fbd429-abae-42c5-b838-4f9ff9484529"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_name = 'gpt2'\n",
    "num_epochs = 20\n",
    "batch_size = 16\n",
    "max_seq_len = 32\n",
    "lr = 5e-5\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "42CY31iUpeQy"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "collate_fn = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qIP4vHCzHcnw"
   },
   "outputs": [],
   "source": [
    "class GetDataset(Dataset):\n",
    "    def __init__(self, root_dir_path, tokenizer, max_seq_len):\n",
    "        self.text = ''\n",
    "        file_count = 0\n",
    "        all_dir_paths = []\n",
    "        for root, _, _ in os.walk(root_dir_path):\n",
    "            if '.git' not in root and root != root_dir_path:\n",
    "                all_dir_paths.append(root)\n",
    "                \n",
    "        for dir_path in all_dir_paths:\n",
    "            for py_file in glob.glob(f'{dir_path}/*.py'):\n",
    "                file_count += 1\n",
    "                self.text += open(py_file, 'r').read()\n",
    "                self.text += '\\n'\n",
    "        print(f'found {file_count} .py files in the given directory!!')\n",
    "\n",
    "        self.words = re.split(' ', self.text) # self.text.split() #\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return 25000 #len(self.words) - self.max_seq_len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        content = self.words[index: index + self.max_seq_len]\n",
    "        tok_con = self.tokenizer(' '.join(content), max_length=self.max_seq_len, truncation=True)\n",
    "        return tok_con"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_wZEmgMhRkeQ",
    "outputId": "1bd2cb67-cb82-4374-8cc6-1ab5ea094ccf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 913 .py files in the given directory!!\n",
      "25000 \"\"\"\n",
      "    Perceptron\n",
      "    w = w + N * (d(k) - y) * x(k)\n",
      "\n",
      "    Using perceptron network for oil analysis, with Measuring of 3 parameters\n",
      "    that represent chemical characteristics we can classify the oil, in p1 or p2\n",
      "    p1 = -1\n",
      "    p2 = 1\n",
      "\"\"\"\n",
      "import random\n",
      "\n",
      "\n",
      "class Perceptron:\n",
      "    def __init__(\n",
      "        self,\n",
      "        sample: list[list[float]],\n",
      "        target: list[int],\n",
      "        learning_rate: float = 0.01,\n",
      "        epoch_number: int = 1000,\n",
      "        bias: float = -1,\n",
      "    ) -> None:\n",
      "        \"\"\"\n",
      "        Initializes a Perceptron network for oil analysis\n",
      "        :param sample: sample dataset of 3 parameters with shape [30,3]\n",
      "        :param target: variable for classification with two possible states -1 or 1\n",
      "        :param learning_rate: learning rate used in optimizing.\n",
      "        :param epoch_number: number of epochs to train network on.\n",
      "        :param bias: bias value for the network.\n",
      "\n",
      "        >>> p = Perceptron([], (0, 1, 2))\n",
      "        Traceback (most recent call last):\n",
      "        ...\n",
      "        ValueErr\n"
     ]
    }
   ],
   "source": [
    "data = GetDataset('Python', tokenizer, max_seq_len)\n",
    "print(len(data), data.text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kz3io76iyZiR",
    "outputId": "e4574545-a2d2-4edd-e499-b1b2d45354c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000 5000 {'input_ids': [220, 220, 3424, 15853, 11841, 58, 12, 16, 60, 628, 220, 220, 220, 611, 18896, 7, 27773, 8, 1222, 352, 25, 198, 220, 220, 220, 220, 220, 220, 220, 3424, 15853, 366], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "train_len = int(len(data) * 0.8)\n",
    "train_data, val_data = random_split(data, [train_len, len(data) - train_len])\n",
    "content = train_data[42]\n",
    "print(len(train_data), len(val_data), content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fa1CEfMMH0dU",
    "outputId": "7e30d024-182e-4ec9-d6d9-a6aa1d34fb01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1250 torch.Size([16, 32]) torch.Size([16, 32])\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=collate_fn, num_workers=2, pin_memory=True)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False, collate_fn=collate_fn, num_workers=2, pin_memory=True)\n",
    "batch = next(iter(train_loader))\n",
    "print(len(train_loader), batch['input_ids'].shape, batch['attention_mask'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KM699RA8ZbQr"
   },
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=lr)\n",
    "num_training_steps = num_epochs * len(train_loader)\n",
    "lr_scheduler = get_scheduler('linear', optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5pZK4xVgYtS4"
   },
   "outputs": [],
   "source": [
    "def loop(model, loader, epoch, is_train):\n",
    "    model.train(is_train)\n",
    "    losses = []\n",
    "    pbar = tqdm(loader, total=len(loader))\n",
    "    for batch in pbar:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.set_grad_enabled(is_train):\n",
    "            outputs = model(**batch, labels=batch['input_ids']) # labels are shifted right inside the model\n",
    "            loss = outputs.loss\n",
    "            losses.append(loss.item())\n",
    "        if is_train:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "        mean_loss = np.mean(losses)\n",
    "        if is_train:\n",
    "            pbar.set_description(f'train: epoch={epoch}, loss={mean_loss:.4f}, ppl={np.exp(mean_loss):.4f}')\n",
    "        else:\n",
    "            pbar.set_description(f' val : epoch={epoch}, loss={mean_loss:.4f}, ppl={np.exp(mean_loss):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zCVU89vVFu_D",
    "outputId": "f27f1318-b6e1-4390-a977-c81aa2cd688a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: epoch=0, loss=1.3109, ppl=3.7096: 100%|██████████| 1250/1250 [03:46<00:00,  5.52it/s]\n",
      " val : epoch=0, loss=0.6430, ppl=1.9022: 100%|██████████| 313/313 [00:16<00:00, 19.48it/s]\n",
      "train: epoch=1, loss=0.6377, ppl=1.8921: 100%|██████████| 1250/1250 [03:48<00:00,  5.46it/s]\n",
      " val : epoch=1, loss=0.4463, ppl=1.5626: 100%|██████████| 313/313 [00:16<00:00, 19.40it/s]\n",
      "train: epoch=2, loss=0.4817, ppl=1.6189: 100%|██████████| 1250/1250 [03:48<00:00,  5.46it/s]\n",
      " val : epoch=2, loss=0.3781, ppl=1.4595: 100%|██████████| 313/313 [00:16<00:00, 19.46it/s]\n",
      "train: epoch=3, loss=0.4107, ppl=1.5078: 100%|██████████| 1250/1250 [03:48<00:00,  5.46it/s]\n",
      " val : epoch=3, loss=0.3520, ppl=1.4219: 100%|██████████| 313/313 [00:16<00:00, 19.40it/s]\n",
      "train: epoch=4, loss=0.3709, ppl=1.4490: 100%|██████████| 1250/1250 [03:48<00:00,  5.46it/s]\n",
      " val : epoch=4, loss=0.3266, ppl=1.3863: 100%|██████████| 313/313 [00:16<00:00, 19.45it/s]\n",
      "train: epoch=5, loss=0.3445, ppl=1.4113: 100%|██████████| 1250/1250 [03:48<00:00,  5.46it/s]\n",
      " val : epoch=5, loss=0.3244, ppl=1.3832: 100%|██████████| 313/313 [00:16<00:00, 19.46it/s]\n",
      "train: epoch=6, loss=0.3257, ppl=1.3850: 100%|██████████| 1250/1250 [03:48<00:00,  5.46it/s]\n",
      " val : epoch=6, loss=0.3204, ppl=1.3777: 100%|██████████| 313/313 [00:16<00:00, 19.46it/s]\n",
      "train: epoch=7, loss=0.3121, ppl=1.3662: 100%|██████████| 1250/1250 [03:48<00:00,  5.46it/s]\n",
      " val : epoch=7, loss=0.3046, ppl=1.3561: 100%|██████████| 313/313 [00:16<00:00, 19.38it/s]\n",
      "train: epoch=8, loss=0.3000, ppl=1.3498: 100%|██████████| 1250/1250 [03:48<00:00,  5.46it/s]\n",
      " val : epoch=8, loss=0.3180, ppl=1.3744: 100%|██████████| 313/313 [00:16<00:00, 19.49it/s]\n",
      "train: epoch=9, loss=0.2924, ppl=1.3397: 100%|██████████| 1250/1250 [03:48<00:00,  5.46it/s]\n",
      " val : epoch=9, loss=0.3170, ppl=1.3730: 100%|██████████| 313/313 [00:16<00:00, 19.43it/s]\n",
      "train: epoch=10, loss=0.2840, ppl=1.3284: 100%|██████████| 1250/1250 [03:48<00:00,  5.46it/s]\n",
      " val : epoch=10, loss=0.3130, ppl=1.3675: 100%|██████████| 313/313 [00:16<00:00, 19.40it/s]\n",
      "train: epoch=11, loss=0.2782, ppl=1.3207: 100%|██████████| 1250/1250 [03:48<00:00,  5.46it/s]\n",
      " val : epoch=11, loss=0.3115, ppl=1.3654: 100%|██████████| 313/313 [00:16<00:00, 19.42it/s]\n",
      "train: epoch=12, loss=0.2731, ppl=1.3140: 100%|██████████| 1250/1250 [03:48<00:00,  5.46it/s]\n",
      " val : epoch=12, loss=0.2990, ppl=1.3486: 100%|██████████| 313/313 [00:16<00:00, 19.41it/s]\n",
      "train: epoch=13, loss=0.2679, ppl=1.3072: 100%|██████████| 1250/1250 [03:48<00:00,  5.46it/s]\n",
      " val : epoch=13, loss=0.2943, ppl=1.3422: 100%|██████████| 313/313 [00:16<00:00, 19.53it/s]\n",
      "train: epoch=14, loss=0.2647, ppl=1.3030: 100%|██████████| 1250/1250 [03:48<00:00,  5.46it/s]\n",
      " val : epoch=14, loss=0.2928, ppl=1.3401: 100%|██████████| 313/313 [00:16<00:00, 19.50it/s]\n",
      "train: epoch=15, loss=0.2605, ppl=1.2975: 100%|██████████| 1250/1250 [03:48<00:00,  5.46it/s]\n",
      " val : epoch=15, loss=0.2933, ppl=1.3409: 100%|██████████| 313/313 [00:16<00:00, 19.48it/s]\n",
      "train: epoch=16, loss=0.2579, ppl=1.2942: 100%|██████████| 1250/1250 [03:48<00:00,  5.46it/s]\n",
      " val : epoch=16, loss=0.3044, ppl=1.3558: 100%|██████████| 313/313 [00:16<00:00, 19.53it/s]\n",
      "train: epoch=17, loss=0.2550, ppl=1.2905: 100%|██████████| 1250/1250 [03:48<00:00,  5.46it/s]\n",
      " val : epoch=17, loss=0.2952, ppl=1.3434: 100%|██████████| 313/313 [00:16<00:00, 19.41it/s]\n",
      "train: epoch=18, loss=0.2521, ppl=1.2868: 100%|██████████| 1250/1250 [03:48<00:00,  5.46it/s]\n",
      " val : epoch=18, loss=0.2909, ppl=1.3376: 100%|██████████| 313/313 [00:16<00:00, 19.46it/s]\n",
      "train: epoch=19, loss=0.2494, ppl=1.2833: 100%|██████████| 1250/1250 [03:48<00:00,  5.46it/s]\n",
      " val : epoch=19, loss=0.2933, ppl=1.3408: 100%|██████████| 313/313 [00:16<00:00, 19.44it/s]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    loop(model, train_loader, epoch, True)\n",
    "    loop(model, val_loader, epoch, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H2lk22Rcd5R6"
   },
   "outputs": [],
   "source": [
    "def generate(text, max_length):\n",
    "    inputs = tokenizer(text, return_tensors='pt')\n",
    "    inputs = inputs['input_ids'].to(device)\n",
    "    outputs = model.generate(inputs, max_length=max_length)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "svQt8kECkfNn",
    "outputId": "485bfe58-fe7a-4975-db9a-cdc6383e19e9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a = {'item1': 1, \n",
      "           '2d' array because it makes the math\n",
      "    # for setting up the table and doing the actual encoding/decoding simpler\n",
      "     table = []\n",
      "\n",
      "    # copy key chars into the table if they are in `alphabet` ignoring duplicates\n",
      "    for char in key.upper():\n",
      "          if char not in table and char in alphabet:\n",
      "               table.append(char)\n",
      "\n",
      "   return table\n",
      "\n",
      "\n",
      "def encode(key: str, words: str) -> str:\n",
      "   table = generate_table(key)\n",
      "   for char in alphabet:\n",
      "       if char not in table and char in alphabet:\n",
      "             table.append(char)\n",
      "\n",
      "   return table\n",
      "\n",
      "\n",
      "def decode(key: str, words: str) -> str:\n",
      "   table = generate_table(key)\n",
      "   for char in alphabet:\n",
      "          if char not in table and char in alphabet:\n",
      "           if char not in table and char in alphabet:\n",
      "            if char not in table and char in alphabet:\n",
      "            if char not in table and char in alphabet:\n",
      "             if char not in table and char in alphabet:\n",
      "             if char not in table and char in alphabet:\n",
      "                      table.append(char)\n",
      "\n",
      "     return table\n",
      "\n",
      "\n",
      "def encode(key: str, words: str) -> str:\n",
      "    table = generate_table(key)\n",
      "    for char in alphabet:\n",
      "            if char not in table and char in alphabet:\n",
      "                table.append(char)\n",
      "\n",
      "    return table\n",
      "\n",
      "\n",
      "def decode(key: str, words: str) -> str:\n",
      "    table = generate_table(key)\n",
      "    for char in alphabet:\n",
      "        if char not in table and char in alphabet:\n",
      "           table.append(char)\n",
      "\n",
      "   return table\n",
      "\n",
      "\n",
      "def encode(key: str, words: str) -> str:\n",
      "  table = generate_table(key)\n",
      "   for char in alphabet:\n",
      "     if char not in table:\n",
      "      table.append(char)\n",
      "\n",
      "   return table\n",
      "\n",
      "\n",
      "def decode(key: str, words: str) -> str:\n",
      "  table = generate_table(key)\n",
      "  for char in alphabet:\n",
      "    if char not in table:\n",
      "    table.append(char)\n",
      "\n",
      "   return table\n",
      "\n",
      "\n",
      "def decode(key: str, words: str) -> str:\n",
      "  table = generate_table(key)\n",
      "   for char in alphabet:\n",
      "    if char not in table:\n",
      "    table.append(char)\n",
      "   return table\n",
      "\n",
      "\n",
      "def decode(key: str, words: str) -> str:\n",
      "  table = generate_table(key)\n",
      "   for char in alphabet:\n",
      "    if char not in table:\n",
      "    if char not in table:\n",
      "     if char not in table:\n",
      "     if char not in table:\n",
      "      if char not in table:\n",
      "      if char not in table:\n",
      "       if char not in table:\n",
      "       if char not in table:\n",
      "        if char not in table:\n",
      "        if char not in table:\n",
      "         if char not in table:\n",
      "              table.append(char)\n",
      "   return table\n",
      "\n",
      "\n",
      "def decode(key: str, words: str) -> str:\n",
      "   table = generate_table(key)\n",
      "   for char in alphabet:\n",
      "       if char not in table:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output = generate('''a = {'item1': 1, \\n''', 1000)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iwFDoHSuS7AJ",
    "outputId": "33565e24-fd42-4876-8019-7cfc26a26eaa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import numpy as np\n",
      "from matplotlib import pyplot as plt\n",
      "\n",
      "\n",
      "def sigmoid(x):\n",
      "    return 1 / (1 + np.exp(-1 * x))\n",
      "\n",
      "\n",
      "class DenseLayer:\n",
      "     \"\"\"\n",
      "    Layers of BP neural network\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(\n",
      "     self, units, activation=None, learning_rate=None, is_input_layer=False\n",
      "    ):\n",
      "         \"\"\"\n",
      "        self.layers = []\n",
      "        self.train_mse = []\n",
      "       self.fig_loss = plt.figure()\n",
      "       self.fig_loss_x = np.dot(pd_k_all, self.train_mse, self.fig_loss.T)\n",
      "         self.xdata = np.asmatrix(pd_k_all, self.train_xdata)\n",
      "          self.fig_loss = plt.figure()\n",
      "          self.fig_loss_xdata = np.asmatrix(pd_k_all, self.train_mse, self.fig_loss.T)\n",
      "           self.fig_loss = plt.figure()\n",
      "            self.fig_loss = plt.figure()\n",
      "            self.train_mse = np.asmatrix(pd_k_all, self.train_mse, self.fig_loss.T)\n",
      "            self.fig_loss_xdata = np.asmatrix(pd_k_all, self.train_mse, self.fig_loss.T)\n",
      "            self.fig_loss_hcipher = plt.figure()\n",
      "            self.back_propagation = np.asmatrix(self.back_propagation)\n",
      "           self.xdata = np.asmatrix(self.xdata[1])\n",
      "         self.train_mse = np.asmatrix(self.xdata[0])\n",
      "        self.fig_loss = plt.figure()\n",
      "       self.fig_loss.T = plt.figure()\n",
      "       plt.xdata = np.dot(pd_k_all, self.train_xdata)\n",
      "      plt.xdata_all = np.dot(pd_k_all, self.fig_loss.T)\n",
      "     plt.xdata_all = np.dot(pd_k_all, self.fig_loss.T)\n",
      "     plt.xdata_all = np.dot(pd_k_all, self.train_xdata)\n",
      "     plt.xdata_all = np.dot(pd_k_all, self.fig_loss.T)\n",
      "     plt.xdata_all = np.dot(pd_k_all, self.xdata)\n",
      "     plt.xdata_all = np.dot(pd_k_all, self.train_xdata)\n",
      "      plt.xdata_all = np.dot(pd_k_all, self.xdata)\n",
      "      plt.xdata = np.dot(pd_k_all, self.train_xdata)\n",
      "       plt.show()\n",
      "       plt.plot(pd_k_all, np.dot(pd_k_all, self.xdata))\n",
      "        plt.plot(pd_all, np.dot(pd_k_all, self.xdata))\n",
      "       plt.plot(pd_all, np.dot(pd_k_all))\n",
      "       plt.xdata = np.dot(pd_k_\n"
     ]
    }
   ],
   "source": [
    "output = generate('''import numpy as np\\n''', 1000)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8SksUrbsS6jV"
   },
   "outputs": [],
   "source": [
    "!transformers-cli login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F-HWWC8qlZJ8"
   },
   "outputs": [],
   "source": [
    "!apt-get install git-lfs\n",
    "!git lfs install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2JdVLwRGlp_p"
   },
   "outputs": [],
   "source": [
    "!git config --global user.email \"<email id>\"\n",
    "!git config --global user.name \"<username>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "INaAdvMWTCHg"
   },
   "outputs": [],
   "source": [
    "save_model_name = 'gpt2-programmer'\n",
    "model.push_to_hub(save_model_name)\n",
    "tokenizer.push_to_hub(save_model_name)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "huggingface_gpt2_programmer.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
