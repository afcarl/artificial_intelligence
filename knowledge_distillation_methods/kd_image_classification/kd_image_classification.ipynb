{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "kd_image_classification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxKrH_0XC2sR"
      },
      "source": [
        "import copy\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch import nn\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms, datasets, models"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZmkDNJSCDFYD",
        "outputId": "aaffc22c-3d6b-49cd-80a1-f8975a756b07"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_epochs = 1\n",
        "img_size = 224\n",
        "batch_size = 64\n",
        "num_classes = 10\n",
        "lr = 3e-4\n",
        "alpha = 0.1\n",
        "temp = 3\n",
        "T = transforms.Compose(\n",
        "    [\n",
        "     transforms.Resize((img_size, img_size)),\n",
        "     transforms.ToTensor()\n",
        "    ]\n",
        ")\n",
        "print(device)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CZFMnk4TD2M-",
        "outputId": "ec9dcbe8-892a-44ac-9eb0-9c68c82e50e9"
      },
      "source": [
        "train_data = datasets.CIFAR10(\"data/\", train=True, download=True, transform=T)\n",
        "val_data = datasets.CIFAR10(\"data/\", train=False, download=True, transform=T)\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
        "x, y = next(iter(train_loader))\n",
        "print(len(train_data), x.shape, y.shape)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "50000 torch.Size([64, 3, 224, 224]) torch.Size([64])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UhJjrZElF9zz"
      },
      "source": [
        "teacher = models.resnet50(num_classes=num_classes).to(device) # big model\n",
        "student = models.resnet18(num_classes=num_classes).to(device) # small model\n",
        "init_student = copy.deepcopy(student)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DoMzMJjzF9xh"
      },
      "source": [
        "ce_loss_fn = nn.CrossEntropyLoss()\n",
        "kld_loss_fn = nn.KLDivLoss(reduction='batchmean')\n",
        "def get_accuracy(preds, y):\n",
        "    preds = preds.argmax(dim=1, keepdim=True)\n",
        "    correct = preds.squeeze(1).eq(y)\n",
        "    acc = correct.sum() / torch.FloatTensor([y.shape[0]]).to(device)\n",
        "    return acc"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2JzcQRAFWXD"
      },
      "source": [
        "def pretrain_loop(net, loader, is_train, optimizer=None):\n",
        "    net.train(is_train)\n",
        "    losses = []\n",
        "    accs = []\n",
        "    pbar = tqdm(loader, total=len(loader))\n",
        "    for x, y in pbar:\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "        with torch.set_grad_enabled(is_train):\n",
        "            preds = net(x)\n",
        "            loss = ce_loss_fn(preds, y)\n",
        "            acc = get_accuracy(preds, y)\n",
        "            losses.append(loss.item())\n",
        "            accs.append(acc.item())\n",
        "\n",
        "        if is_train:\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        pbar.set_description(f'epoch={epoch}, train={int(is_train)}')\n",
        "        pbar.set_postfix(loss=f'{np.mean(losses):.4f}', acc=f'{np.mean(accs):.4f}')\n",
        "\n",
        "def distill_loop(teacher, student, loader, is_train, optimizer=None):\n",
        "    teacher.eval()\n",
        "    student.train(is_train)\n",
        "    losses = []\n",
        "    accs = []\n",
        "    pbar = tqdm(loader, total=len(loader))\n",
        "    for x, y in pbar:\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "        with torch.no_grad():\n",
        "            teacher_preds = teacher(x)\n",
        "            \n",
        "        with torch.set_grad_enabled(is_train):\n",
        "            student_preds = student(x)\n",
        "            student_loss = ce_loss_fn(student_preds, y)\n",
        "            acc = get_accuracy(student_preds, y)\n",
        "            student_preds = (student_preds / temp).softmax(-1)\n",
        "            teacher_preds = (teacher_preds / temp).softmax(-1)\n",
        "            distillation_loss = kld_loss_fn(student_preds, teacher_preds)\n",
        "            loss = alpha * student_loss + (1 - alpha) * distillation_loss\n",
        "            losses.append(loss.item())\n",
        "            accs.append(acc.item())\n",
        "\n",
        "        if is_train:\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        pbar.set_description(f'epoch={epoch}, train={int(is_train)}')\n",
        "        pbar.set_postfix(loss=f'{np.mean(losses):.4f}', acc=f'{np.mean(accs):.4f}')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yhUaPbBBHNCL",
        "outputId": "6b55e7fd-e2ba-4d48-f410-d66ebb899430"
      },
      "source": [
        "# training the teacher model\n",
        "teacher_optimizer = torch.optim.Adam(teacher.parameters(), lr=lr)\n",
        "for epoch in range(n_epochs):\n",
        "    pretrain_loop(teacher, train_loader, True, teacher_optimizer)\n",
        "    pretrain_loop(teacher, val_loader, False)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch=0, train=1: 100%|██████████| 782/782 [08:32<00:00,  1.53it/s, acc=0.4597, loss=1.4821]\n",
            "epoch=0, train=0: 100%|██████████| 157/157 [00:31<00:00,  4.92it/s, acc=0.5373, loss=1.3888]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "umrvOlMdLgaG",
        "outputId": "7bc4668e-c70f-4997-9b45-c937699c56ed"
      },
      "source": [
        "# training a clone of the student model, just to compare this with the distilled one\n",
        "init_student_optimizer = torch.optim.Adam(init_student.parameters(), lr=lr)\n",
        "for epoch in range(n_epochs):\n",
        "    pretrain_loop(init_student, train_loader, True, init_student_optimizer)\n",
        "    pretrain_loop(init_student, val_loader, False)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch=0, train=1: 100%|██████████| 782/782 [02:33<00:00,  5.11it/s, acc=0.5580, loss=1.2202]\n",
            "epoch=0, train=0: 100%|██████████| 157/157 [00:16<00:00,  9.34it/s, acc=0.6309, loss=1.0518]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mYQ2RwwAJnCG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5324d6d2-4b64-4e39-f137-5fe63cb7e0b4"
      },
      "source": [
        "# freezing the teacher model's paramters for distillation process\n",
        "for params in teacher.parameters():\n",
        "    params.requires_grad = False\n",
        "\n",
        "# distilling the teacher model\n",
        "distill_optimizer = torch.optim.Adam(student.parameters(), lr=lr)\n",
        "for epoch in range(n_epochs):\n",
        "    distill_loop(teacher, student, train_loader, True, distill_optimizer)\n",
        "    distill_loop(teacher, student, val_loader, False)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch=0, train=1: 100%|██████████| 782/782 [05:10<00:00,  2.52it/s, acc=0.5448, loss=-1.7678]\n",
            "epoch=0, train=0: 100%|██████████| 157/157 [00:41<00:00,  3.74it/s, acc=0.6291, loss=-1.8027]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ayjCdTegTH22"
      },
      "source": [
        "# now the distilled student model is just as good as the pretrained teacher model"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}